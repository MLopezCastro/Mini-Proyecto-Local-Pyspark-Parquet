
---

# ðŸš€ Mini-Proyecto Local PySpark â€“ Procesamiento y Escritura Parquet

## ðŸ§  DescripciÃ³n general

Este proyecto demuestra cÃ³mo ejecutar **Apache Spark localmente en Windows** utilizando **PySpark**, sin necesidad de un clÃºster real.
El objetivo fue crear un **pipeline completo de procesamiento de datos**: leer un CSV sintÃ©tico, transformarlo, filtrar y escribir los resultados en formato **Parquet particionado y comprimido**.

AdemÃ¡s, se resolviÃ³ el clÃ¡sico error de entorno en Windows relacionado con **Hadoop (`winutils.exe`)**, lo que permitiÃ³ que Spark escribiera correctamente los archivos de salida en disco.

---

## ðŸ§© Estructura del proyecto

```
Mini-Proyecto-Local-Pyspark-Parquet/
â”œâ”€ .venv/                     â† entorno virtual
â”œâ”€ data/
â”‚  â””â”€ ventas.csv              â† dataset sintÃ©tico generado automÃ¡ticamente
â”œâ”€ logs/
â”‚  â””â”€ final.txt               â† logs y tiempos de ejecuciÃ³n
â”œâ”€ output/
â”‚  â””â”€ ventas_agg/             â† salida Parquet particionada
â”œâ”€ nyc_simple.py              â† script principal
â”œâ”€ EXPLICACION_TECNICA.md     â† documentaciÃ³n tÃ©cnica
â”œâ”€ EXPLICACION_GENERAL.md     â† documentaciÃ³n conceptual
â””â”€ README.md
```

---

## âš™ï¸ PreparaciÃ³n del entorno

### 1ï¸âƒ£ Crear y activar entorno virtual

```powershell
py -3.11 -m venv .venv
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\.venv\Scripts\Activate.ps1
```

### 2ï¸âƒ£ Instalar dependencias

```powershell
python -m pip install --upgrade pip
python -m pip install pyspark==3.5.1 pandas pyarrow faker
```

---

## ðŸ§± ConfiguraciÃ³n de Hadoop en Windows

Spark requiere `winutils.exe` para crear y manejar archivos locales en sistemas Windows.
Sin este archivo, aparece el error:

```
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset
```

### ðŸ”¹ SoluciÃ³n aplicada

1. Se creÃ³ la carpeta:

   ```
   C:\hadoop\bin\
   ```
2. Se descargÃ³ el ejecutable `winutils.exe` compatible con Hadoop 3.3.x
   (por ejemplo, desde el repositorio oficial de Steve Loughran o conda-forge).
3. Se configuraron las variables de entorno en PowerShell:

```powershell
$env:HADOOP_HOME = "C:\hadoop"
$env:PATH = "C:\hadoop\bin;" + $env:PATH
```

4. Se verificÃ³ la instalaciÃ³n ejecutando:

```powershell
C:\hadoop\bin\winutils.exe
```

Debe devolver la lista de comandos disponibles (no un error).

---

## ðŸ§© EjecuciÃ³n del script principal

### ðŸ§¾ Archivo: `nyc_simple.py`

```python
import os, time
from faker import Faker
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month, sum as _sum, to_date

# ---- ConfiguraciÃ³n de rutas ----
BASE = os.getcwd()
CSV = os.path.join(BASE, "data", "ventas.csv")
OUT = os.path.join(BASE, "output", "ventas_agg")

os.makedirs(os.path.dirname(CSV), exist_ok=True)
os.makedirs(OUT, exist_ok=True)

print(f"[DEBUG] CSV : {CSV}")
print(f"[DEBUG] OUT : {OUT}")

# ---- Generar CSV sintÃ©tico si no existe ----
if not os.path.exists(CSV):
    print("[INFO] Generando datos sintÃ©ticos...")
    fake = Faker()
    data = [
        (fake.uuid4(), fake.random_element(["Norte","Sur","Este","Oeste"]),
         fake.date_between(start_date="-1y", end_date="today"),
         round(fake.pyfloat(left_digits=3, right_digits=2, positive=True) * 100, 2))
        for _ in range(300_000)
    ]
    df_fake = pd.DataFrame(data, columns=["cliente_id","region","fecha","monto"])
    df_fake.to_csv(CSV, index=False)
    print("[INFO] CSV generado.")

# ---- Crear SparkSession ----
spark = (
    SparkSession.builder
        .master("local[*]")
        .appName("MiniProyectoLocal")
        .config("spark.sql.shuffle.partitions", "12")
        .config("spark.hadoop.io.native.lib.available", "false")
        .getOrCreate()
)

# ---- Leer CSV + tiempo ----
t0 = time.perf_counter()
df = spark.read.csv(CSV, header=True, inferSchema=True)
print(f"[CSV] filas={df.count():,} tiempo_lectura={time.perf_counter()-t0:.2f}s")

df = df.withColumn("fecha", to_date("fecha")) \
       .withColumn("anio", year("fecha")) \
       .withColumn("mes", month("fecha"))

# ---- Filtro ----
df_filtrado = df.filter(col("monto") > 500)

# ---- AgregaciÃ³n ----
agg = df_filtrado.groupBy("region", "anio", "mes").agg(_sum("monto").alias("monto_total"))

# ---- Escritura Parquet particionada ----
agg.write.mode("overwrite") \
         .partitionBy("anio", "mes") \
         .option("compression", "snappy") \
         .parquet(OUT)
print(f"[WRITE] â†’ {OUT}")

# ---- Leer Parquet + benchmark ----
t1 = time.perf_counter()
parq = spark.read.parquet(OUT)
print(f"[PARQUET] filas={parq.count():,} tiempo_lectura={time.perf_counter()-t1:.2f}s")

# ---- Plan de ejecuciÃ³n ----
agg.explain(True)

spark.stop()
```

---

## ðŸ§ª EjecuciÃ³n y verificaciÃ³n

Ejecutar desde PowerShell dentro del entorno virtual:

```powershell
python .\nyc_simple.py
```

Salida esperada (aproximada):

```
[DEBUG] CSV : C:\...\data\ventas.csv
[DEBUG] OUT : C:\...\output\ventas_agg
[CSV] filas=300,000 tiempo_lectura=6.04s
[WRITE] â†’ C:\...\output\ventas_agg
[PARQUET] filas=300,000 tiempo_lectura=1.52s
```

Y en el explorador de VS Code se verÃ¡:

```
output/
â””â”€ ventas_agg/
   â”œâ”€ anio=2023/
   â”‚  â”œâ”€ mes=1/
   â”‚  â”œâ”€ mes=2/
   â”‚  â””â”€ ...
   â”œâ”€ _SUCCESS
```

---

## ðŸ§° ReejecuciÃ³n limpia

Para borrar resultados previos y correr el pipeline desde cero:

```powershell
Remove-Item -Recurse -Force .\output\ventas_agg -ErrorAction SilentlyContinue
python .\nyc_simple.py
```

---

## ðŸ§© .gitignore recomendado

```
.venv/
data/*.csv
output/**
logs/**
*.log
```

---

## ðŸ§  ConclusiÃ³n

Este proyecto muestra paso a paso cÃ³mo ejecutar **PySpark localmente en Windows**, desde la configuraciÃ³n del entorno hasta la escritura optimizada en **Parquet**.
Se resolviÃ³ el error `HADOOP_HOME unset` configurando `winutils.exe`, logrando que Spark pueda manejar archivos sin depender de Linux o un clÃºster remoto.

El resultado final es un pipeline reproducible, portable y tÃ©cnicamente sÃ³lido, que demuestra dominio prÃ¡ctico de:

* **PySpark SQL**
* **Particionamiento Parquet**
* **ConfiguraciÃ³n Hadoop local**
* **Comparativa de rendimiento entre CSV y Parquet**

---


