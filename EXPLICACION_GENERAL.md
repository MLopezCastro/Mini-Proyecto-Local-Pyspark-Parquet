# üß† Mini-Proyecto PySpark ‚Äì Explicaci√≥n Conceptual

## 1Ô∏è‚É£ ¬øQu√© es Apache Spark?

**Apache Spark** es un motor de procesamiento de datos distribuido.
Permite leer, transformar y analizar grandes vol√∫menes de datos en paralelo, incluso si est√°n repartidos en varias m√°quinas.

En este proyecto, se utiliz√≥ Spark **localmente**, simulando un mini-cluster dentro de una sola PC.

---

## 2Ô∏è‚É£ ¬øQu√© hace Spark en la pr√°ctica?

Spark trabaja con m√≥dulos:

* **Spark SQL:** trabaja con tablas (DataFrames) y permite usar SQL.
* **Spark Core:** maneja la ejecuci√≥n distribuida (los hilos y particiones).
* **Spark Streaming, MLlib y GraphX:** m√≥dulos adicionales que no usamos aqu√≠.

Nosotros trabajamos con **Spark SQL**, manipulando DataFrames como si fueran tablas o planillas.

---

## 3Ô∏è‚É£ La analog√≠a simple

Imagin√° que ten√©s un Excel gigante con millones de filas.
Si lo abr√≠s con pandas, se carga todo en memoria y puede colgarse.

Spark, en cambio:

* Divide el archivo en **particiones**.
* Procesa cada parte **en paralelo**.
* Une los resultados al final.

Esto lo hace ideal para trabajar con **big data**, aunque tambi√©n puede usarse localmente como hicimos ac√°.

---

## 4Ô∏è‚É£ Qu√© hace este proyecto paso a paso

1. **Crea un CSV sint√©tico** con 300.000 registros (clientes, regiones, fechas, montos).
2. **Lee el CSV con Spark**, infiriendo los tipos de datos.
3. **Transforma los datos** agregando columnas de `anio` y `mes`.
4. **Filtra** registros con `monto > 500`.
5. **Agrupa** por `region`, `anio` y `mes`, sumando los montos.
6. **Escribe el resultado en formato Parquet**, comprimido y **particionado** por a√±o y mes.
7. **Compara rendimiento** entre CSV y Parquet.
8. **Muestra el plan de ejecuci√≥n (EXPLAIN)** para ver c√≥mo Spark procesa las tareas internamente.

---

## 5Ô∏è‚É£ ¬øQu√© es el formato Parquet?

**Parquet** es un formato binario columnar (guarda los datos por columnas, no por filas).

Ventajas:

* Mucho m√°s r√°pido que CSV.
* Mucho m√°s liviano.
* Permite leer solo columnas o carpetas relevantes (partition pruning).

Ejemplo de estructura generada:

```
output/
ventas_agg/
anio=2023/
mes=1/
mes=2/
...
```

---

## 6Ô∏è‚É£ ¬øPor qu√© tanto problema con `winutils.exe`?

Spark usa internamente funciones de **Hadoop** para manejar archivos.
En Linux, esas utilidades vienen integradas.
En Windows, no.

Por eso Spark necesita que exista el archivo `winutils.exe` dentro de `C:\hadoop\bin`.

Las variables necesarias son:

```powershell
$env:HADOOP_HOME = "C:\hadoop"
$env:PATH = "C:\hadoop\bin;" + $env:PATH
```

---

## 7Ô∏è‚É£ Qu√© lograste

‚úÖ Crear y activar un entorno virtual con PySpark.
‚úÖ Ejecutar un pipeline local de Spark.
‚úÖ Leer, transformar y guardar datos.
‚úÖ Generar resultados en formato Parquet.
‚úÖ Medir rendimiento.
‚úÖ Comprender c√≥mo Spark maneja datos en paralelo.

---

## 8Ô∏è‚É£ C√≥mo podr√≠as explicarlo a otra persona

> "Constru√≠ un pipeline local con PySpark para procesar datos de ventas.
> Gener√© un dataset sint√©tico, lo transform√© y lo export√© a formato Parquet particionado por a√±o y mes.
> Aprend√≠ c√≥mo Spark maneja la paralelizaci√≥n, el formato Parquet y la configuraci√≥n local de Hadoop en Windows."

