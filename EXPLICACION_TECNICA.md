
---

### âœ… VersiÃ³n corregida de `EXPLICACION_TECNICA.md`

```markdown
# âš™ï¸ Mini-Proyecto Local PySpark â€“ ExplicaciÃ³n TÃ©cnica

## ðŸ§© Estructura del proyecto

```

Mini-Proyecto-Local-Pyspark-Parquet/
â”œâ”€ .venv/                     â† entorno virtual
â”œâ”€ data/
â”‚  â””â”€ ventas.csv              â† datos sintÃ©ticos
â”œâ”€ logs/
â”‚  â””â”€ final.txt               â† logs o tiempos de ejecuciÃ³n
â”œâ”€ output/
â”‚  â””â”€ ventas_agg/             â† salida Parquet particionada
â”œâ”€ nyc_simple.py              â† script principal
â””â”€ README.md

````

---

## âš™ï¸ PreparaciÃ³n del entorno

### 1ï¸âƒ£ Crear y activar entorno virtual

```powershell
py -3.11 -m venv .venv
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\.venv\Scripts\Activate.ps1
````

### 2ï¸âƒ£ Instalar dependencias

```powershell
python -m pip install --upgrade pip
python -m pip install pyspark==3.5.1 pandas pyarrow faker
```

### 3ï¸âƒ£ Configurar Hadoop en Windows

Spark requiere el binario `winutils.exe` para manejar archivos locales.
Descargalo y colocalo en `C:\hadoop\bin\winutils.exe`.

Luego, en PowerShell:

```powershell
$env:HADOOP_HOME = "C:\hadoop"
$env:PATH = "C:\hadoop\bin;" + $env:PATH
```

ComprobÃ¡ que funciona:

```powershell
C:\hadoop\bin\winutils.exe
```

Debe mostrar la ayuda, no un error.

---

## ðŸ§± Script principal (`nyc_simple.py`)

### ðŸ”¹ 1. ConfiguraciÃ³n inicial

Define rutas relativas (`BASE`, `CSV`, `OUT`), crea carpetas y usa `Faker` para generar datos si no existen.

### ðŸ”¹ 2. Crear SparkSession

```python
spark = (
    SparkSession.builder
        .master("local[*]")
        .appName("MiniProyectoLocal")
        .config("spark.sql.shuffle.partitions", "12")
        .config("spark.hadoop.io.native.lib.available", "false")
        .getOrCreate()
)
```

* `local[*]`: usa todos los cores del equipo.
* `spark.sql.shuffle.partitions`: reduce particiones por defecto (200 â†’ 12).
* `native.lib.available=false`: evita intentar librerÃ­as nativas ausentes en Windows.

### ðŸ”¹ 3. Lectura CSV

```python
df = spark.read.csv(CSV, header=True, inferSchema=True)
df.printSchema()
```

### ðŸ”¹ 4. Transformaciones

```python
df2 = df.withColumn("fecha", to_date("fecha")) \
        .withColumn("anio", year("fecha")) \
        .withColumn("mes", month("fecha"))
df_filtrado = df2.filter(col("monto") > 500)
```

### ðŸ”¹ 5. AgregaciÃ³n

```python
agg = df_filtrado.groupBy("region", "anio", "mes") \
                 .agg(_sum("monto").alias("monto_total"))
```

### ðŸ”¹ 6. Escritura Parquet

```python
agg.write.mode("overwrite") \
         .partitionBy("anio","mes") \
         .option("compression","snappy") \
         .parquet(OUT)
```

Crea una carpeta por aÃ±o y mes:

```
output/ventas_agg/anio=2023/mes=1/
```

### ðŸ”¹ 7. Benchmark

Lee el Parquet y mide el tiempo comparado con CSV:

```python
parq = spark.read.parquet(OUT)
parq.count()
```

### ðŸ”¹ 8. Plan de ejecuciÃ³n

```python
agg.explain(True)
```

Muestra el plan lÃ³gico y fÃ­sico (`Exchange`, `HashAggregate`, etc.).

---

## ðŸ§ª Validaciones

* Archivos `.snappy.parquet` creados correctamente.
* Archivo `_SUCCESS` presente.
* Plan fÃ­sico con `Exchange hashpartitioning`.
* Parquet mucho mÃ¡s rÃ¡pido de leer que CSV.

---

## ðŸ§° ReejecuciÃ³n limpia

```powershell
Remove-Item -Recurse -Force .\output\ventas_agg -ErrorAction SilentlyContinue
python .\nyc_simple.py
```

---

## ðŸš« Errores comunes

| Error                  | Causa                          | SoluciÃ³n                                |
| ---------------------- | ------------------------------ | --------------------------------------- |
| `HADOOP_HOME unset`    | No estÃ¡ configurado `winutils` | Setear `$env:HADOOP_HOME` y `$env:PATH` |
| `UnsatisfiedLinkError` | Binario winutils incorrecto    | Descargar versiÃ³n Hadoop 3.3.x          |
| `Parquet no aparece`   | OUT apuntaba a `C:\spark_out`  | Corregir a `.\\output\\ventas_agg`      |
| `Java not found`       | Java ausente o PATH roto       | Instalar JDK 11 o superior              |

---

## ðŸ§© .gitignore recomendado

```
.venv/
data/*.csv
output/**
logs/**
*.log
```

---

## âœ… ConclusiÃ³n

Este proyecto demuestra cÃ³mo correr **PySpark localmente en Windows**,
desde la configuraciÃ³n de entorno hasta la escritura optimizada en **Parquet particionado**.

El resultado es un pipeline reproducible, portable y que muestra los fundamentos del procesamiento distribuido de datos con Spark SQL.

```

---


```
